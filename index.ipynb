{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'w': 0, ' ': 1, 'd': 2, 'h': 3, 'r': 4, 'o': 5, 'e': 6, 'l': 7},\n",
       " {0: 'w', 1: ' ', 2: 'd', 3: 'h', 4: 'r', 5: 'o', 6: 'e', 7: 'l'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"hello world\"\n",
    "chars = list(set(data))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "chars_to_indices = {char: i for i, char in enumerate(chars)}\n",
    "indices_to_chars = {i: char for i, char in enumerate(chars)}\n",
    "\n",
    "chars_to_indices, indices_to_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN configs\n",
    "embedding_size = vocab_size\n",
    "hidden_size = 64\n",
    "output_size = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices\n",
    "Wxh = np.random.randn(embedding_size, hidden_size)\n",
    "Whh = np.random.randn(hidden_size, hidden_size)\n",
    "bh = np.random.randn(1, hidden_size)\n",
    "\n",
    "Why = np.random.randn(hidden_size, output_size)\n",
    "by = np.random.randn(1, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    return e_z / e_z.sum(axis=1)\n",
    "\n",
    "def rnn(x_t, h_prev):\n",
    "    assert x_t.shape == (1, embedding_size)\n",
    "    assert h_prev.shape == (1, hidden_size)\n",
    "\n",
    "    h_next = np.tanh(x_t @ Wxh + h_prev @ Whh + bh)\n",
    "    probs = softmax(h_next @ Why + by)\n",
    "    \n",
    "    return probs, h_next\n",
    "\n",
    "def sample(char, n):\n",
    "    # sample rnn n times starting with the first char\n",
    "    x = np.zeros((1, vocab_size))\n",
    "    idx = chars_to_indices[char]\n",
    "    x[:, idx] = 1 # one hot encoding\n",
    "    h =  np.zeros((1, hidden_size)) # hidden state\n",
    "    idxes = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        probs, h = rnn(x, h) # xt, hprev -> rnn -> probs, hnext\n",
    "        idx = np.random.choice(vocab_size, p=probs.ravel()) # .ravel returns a 1d array\n",
    "        \n",
    "        x = np.zeros((1, vocab_size))\n",
    "        x[:, idx] = 1\n",
    "        idxes.append(idx)\n",
    "\n",
    "    chars = \"\".join([indices_to_chars[i] for i in idxes])\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 8), (1, 64))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t = np.random.randn(1, embedding_size)\n",
    "h_prev = np.random.randn(1, hidden_size)\n",
    "\n",
    "probs, h_next = rnn(x_t, h_prev)\n",
    "probs.shape, h_next.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3, 6, 7, 7, 5, 1, 0, 5, 4, 7], [6, 7, 7, 5, 1, 0, 5, 4, 7, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = [chars_to_indices[c] for c in data[:-1]]\n",
    "target = [chars_to_indices[c] for c in data[1:]]\n",
    "\n",
    "inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hwor'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(\"h\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loss]: 5.122753882063285\n",
      "[Loss]: 11.095246920650402\n",
      "[Loss]: 8.554840069864063\n",
      "[Loss]: 4.226851572140599\n",
      "[Loss]: 6.282191211155975\n",
      "[Loss]: 19.620474084043614\n",
      "[Loss]: 37.057248433580725\n",
      "[Loss]: 28.842950577724288\n",
      "[Loss]: 0.3405652928045862\n",
      "[Loss]: 0.45549371931877686\n",
      "[Loss]: 7.339589587308039\n",
      "[Loss]: 11.553390264808915\n",
      "[Loss]: 6.685820575268527\n",
      "[Loss]: 2.0277017363159366\n",
      "[Loss]: 4.464625594727593\n",
      "[Loss]: 5.538300242719268\n",
      "[Loss]: 8.54128007689768\n",
      "[Loss]: 11.7598644598439\n",
      "[Loss]: 0.6400021888355423\n",
      "[Loss]: 10.208360505806802\n",
      "[Loss]: 1.162722630227682\n",
      "[Loss]: 0.23649887117646465\n",
      "[Loss]: 0.26332940426910817\n",
      "[Loss]: 0.25785018465312637\n",
      "[Loss]: 0.4956125698750249\n",
      "[Loss]: 3.6033541483492537\n",
      "[Loss]: 4.8530544888802245\n",
      "[Loss]: 4.23885081102703\n",
      "[Loss]: 5.19579297376276\n",
      "[Loss]: 9.993900973867717\n",
      "[Loss]: 0.634078454279546\n",
      "[Loss]: 0.06591927694338401\n",
      "[Loss]: 0.06543873268919828\n",
      "[Loss]: 0.3192845189321185\n",
      "[Loss]: 3.46441430662075\n",
      "[Loss]: 0.6070381489733823\n",
      "[Loss]: 2.8859862767660425\n",
      "[Loss]: 1.5977579491258285\n",
      "[Loss]: 6.516015507848213\n",
      "[Loss]: 0.08415319877362525\n",
      "[Loss]: 2.0367290724405462\n",
      "[Loss]: 8.059254410059639\n",
      "[Loss]: 1.0481908752873172\n",
      "[Loss]: 0.4468591919275117\n",
      "[Loss]: 0.15331979781148822\n",
      "[Loss]: 0.17158848252069658\n",
      "[Loss]: 0.19332784708956355\n",
      "[Loss]: 0.1975131104791724\n",
      "[Loss]: 0.2606514069424314\n",
      "[Loss]: 0.25567730732405064\n",
      "[Loss]: 10.40082481938843\n",
      "[Loss]: 1.620933049111993\n",
      "[Loss]: 0.4013564051420815\n",
      "[Loss]: 11.914798054383851\n",
      "[Loss]: 12.651767663388071\n",
      "[Loss]: 13.193197115721805\n",
      "[Loss]: 2.5368748469241313\n",
      "[Loss]: 7.421425972621236\n",
      "[Loss]: 4.319788363631532\n",
      "[Loss]: 7.043632615193912\n",
      "[Loss]: 6.5552232392659695\n",
      "[Loss]: 0.3175966319814776\n",
      "[Loss]: 9.988340620934821\n",
      "[Loss]: 1.3659020284677004\n",
      "[Loss]: 6.225695938265094\n",
      "[Loss]: 3.8174322958312232\n",
      "[Loss]: 15.423573935676185\n",
      "[Loss]: 0.15465597595935082\n",
      "[Loss]: 0.16116466666259374\n",
      "[Loss]: 0.16519561111465195\n",
      "[Loss]: 0.16975545834370395\n",
      "[Loss]: 0.1947303805117978\n",
      "[Loss]: 0.21763265486704272\n",
      "[Loss]: 0.19594733080782747\n",
      "[Loss]: 0.1787950141315226\n",
      "[Loss]: 0.18395001359265534\n",
      "[Loss]: 0.23303165046281632\n",
      "[Loss]: 0.26945224551705776\n",
      "[Loss]: 0.2858611418353082\n",
      "[Loss]: 0.43054275058259156\n",
      "[Loss]: 0.1740925966131071\n",
      "[Loss]: 0.5513464316954992\n",
      "[Loss]: 0.0931567830506888\n",
      "[Loss]: 0.08961962257975717\n",
      "[Loss]: 0.09929508670404558\n",
      "[Loss]: 0.08493127513690232\n",
      "[Loss]: 0.23467395827388\n",
      "[Loss]: 0.08942836065330506\n",
      "[Loss]: 0.059287618214377905\n",
      "[Loss]: 0.06782442508767325\n",
      "[Loss]: 0.09913409510392113\n",
      "[Loss]: 0.15551035828224988\n",
      "[Loss]: 2.7381149619396674\n",
      "[Loss]: 0.07582590616258272\n",
      "[Loss]: 0.1191711773598003\n",
      "[Loss]: 0.06338114202092889\n",
      "[Loss]: 0.06580247026255691\n",
      "[Loss]: 0.06828029950268827\n",
      "[Loss]: 0.06738602595749135\n",
      "[Loss]: 0.07314670082654279\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-2\n",
    "for e in range(100):\n",
    "    # compute loss\n",
    "    loss = 0\n",
    "    xs, ps, hs = {}, {}, {}\n",
    "    h = np.zeros((1, hidden_size))\n",
    "    for t in range(len(inp)):\n",
    "        idx = inp[t]\n",
    "        x = np.zeros((1, vocab_size))\n",
    "        x[:, idx] = 1\n",
    "        probs, h = rnn(x, h)\n",
    "        xs[t] = x\n",
    "        ps[t] = probs # save probs (we'll use this in backprop)\n",
    "        hs[t] = h\n",
    "\n",
    "        # cross entropy\n",
    "        pred = probs[0, target[t]]\n",
    "        loss += -np.log(pred)\n",
    "\n",
    "    # backprop, calculate gradients\n",
    "    dL_dWxh = np.zeros_like(Wxh)\n",
    "    dL_dWhh = np.zeros_like(Whh)\n",
    "    dL_dbh = np.zeros_like(bh)\n",
    "    dL_Why = np.zeros_like(Why)\n",
    "    dL_dby = np.zeros_like(by)\n",
    "    dF_dh = np.zeros((1, hidden_size))\n",
    "\n",
    "    for t in reversed(range(len(inp))):\n",
    "        dL_dz2 = np.copy(ps[t])\n",
    "        target_idx = target[t]\n",
    "        dL_dz2[:, target_idx] -= 1\n",
    "\n",
    "        # 2nd layer\n",
    "        dL_Why += dL_dz2 * hs[t].T\n",
    "        dL_dby += dL_dz2\n",
    "\n",
    "        # 1st layer\n",
    "        dh_dz1 = 1 - hs[t] ** 2\n",
    "        dL_dh = (dL_dz2 @ Why.T) + dF_dh \n",
    "\n",
    "        dL_dWxh += dL_dh * dh_dz1 * xs[t].T\n",
    "        dL_dWhh += dL_dh * dh_dz1 * hs[t].T\n",
    "        dL_dbh += dL_dh * dh_dz1\n",
    "\n",
    "        dF_dh = dL_dh * (dh_dz1 @ Whh.T)\n",
    "\n",
    "    # clip gradients\n",
    "    for gradient in [dL_dWxh, dL_dWhh, dL_Why, dL_dbh, dL_dby]:\n",
    "        np.clip(gradient, -1, 1, out=gradient)\n",
    "\n",
    "    # gradient descent\n",
    "    for weights, gradient in zip([Wxh, Whh, Why, bh, by], [dL_dWxh, dL_dWhh, dL_Why, dL_dbh, dL_dby]):\n",
    "        weights -= lr * gradient\n",
    "\n",
    "    # break\n",
    "    \n",
    "    print(f\"[Loss]: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ello world'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(\"h\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "core-JXcGQI7U-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
