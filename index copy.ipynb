{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'d': 0, 'h': 1, 'r': 2, 'l': 3, 'w': 4, 'o': 5, 'e': 6, ' ': 7},\n",
       " {0: 'd', 1: 'h', 2: 'r', 3: 'l', 4: 'w', 5: 'o', 6: 'e', 7: ' '})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"hello world\"\n",
    "chars = list(set(data))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "chars_to_indices = {char: i for i, char in enumerate(chars)}\n",
    "indices_to_chars = {i: char for i, char in enumerate(chars)}\n",
    "\n",
    "chars_to_indices, indices_to_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN configs\n",
    "embedding_size = vocab_size\n",
    "hidden_size = 64\n",
    "output_size = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices\n",
    "Wxh = np.random.randn(hidden_size, embedding_size)\n",
    "Whh = np.random.randn(hidden_size, hidden_size)\n",
    "bh = np.random.randn(1, hidden_size)\n",
    "\n",
    "Why = np.random.randn(output_size, hidden_size)\n",
    "by = np.random.randn(1, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    return e_z / e_z.sum(axis=1)\n",
    "\n",
    "def rnn(x_t, h_prev):\n",
    "    assert x_t.shape == (1, embedding_size)\n",
    "    assert h_prev.shape == (1, hidden_size)\n",
    "\n",
    "    h_next = np.tanh(x_t @ Wxh.T + h_prev @ Whh.T + bh)\n",
    "    probs = softmax(h_next @ Why.T + by)\n",
    "    \n",
    "    return probs, h_next\n",
    "\n",
    "def sample(char, n):\n",
    "    # sample rnn n times starting with the first char\n",
    "    x = np.zeros((1, vocab_size))\n",
    "    idx = chars_to_indices[char]\n",
    "    x[:, idx] = 1 # one hot encoding\n",
    "    h =  np.zeros((1, hidden_size)) # hidden state\n",
    "    idxes = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        probs, h = rnn(x, h) # xt, hprev -> rnn -> probs, hnext\n",
    "        idx = np.random.choice(vocab_size, p=probs.ravel()) # .ravel returns a 1d array\n",
    "        \n",
    "        x = np.zeros((1, vocab_size))\n",
    "        x[:, idx] = 1\n",
    "        idxes.append(idx)\n",
    "\n",
    "    chars = \"\".join([indices_to_chars[i] for i in idxes])\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 8), (1, 64))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t = np.random.randn(1, embedding_size)\n",
    "h_prev = np.random.randn(1, hidden_size)\n",
    "\n",
    "probs, h_next = rnn(x_t, h_prev)\n",
    "probs.shape, h_next.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 6, 3, 3, 5, 7, 4, 5, 2, 3], [6, 3, 3, 5, 7, 4, 5, 2, 3, 0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = [chars_to_indices[c] for c in data[:-1]]\n",
    "target = [chars_to_indices[c] for c in data[1:]]\n",
    "\n",
    "inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'howw'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(\"h\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loss]: 0.005866875949557311\n",
      "[Loss]: 0.005850452374357534\n",
      "[Loss]: 0.00583414628022173\n",
      "[Loss]: 0.0058179574300164395\n",
      "[Loss]: 0.00580188557816735\n",
      "[Loss]: 0.005785930469001028\n",
      "[Loss]: 0.005770091835390829\n",
      "[Loss]: 0.005754369397773713\n",
      "[Loss]: 0.005738762863612125\n",
      "[Loss]: 0.005723271927346474\n",
      "[Loss]: 0.005707896270914847\n",
      "[Loss]: 0.005692635564872782\n",
      "[Loss]: 0.005677489470162945\n",
      "[Loss]: 0.005662457640572737\n",
      "[Loss]: 0.005647539725873965\n",
      "[Loss]: 0.005632735375644362\n",
      "[Loss]: 0.00561804424373372\n",
      "[Loss]: 0.00560346599327266\n",
      "[Loss]: 0.005589000302104848\n",
      "[Loss]: 0.0055746468684385935\n",
      "[Loss]: 0.0055604054164409625\n",
      "[Loss]: 0.005546275701422274\n",
      "[Loss]: 0.005532257514172846\n",
      "[Loss]: 0.005518350683925124\n",
      "[Loss]: 0.0055045550793601725\n",
      "[Loss]: 0.005490870607045043\n",
      "[Loss]: 0.005477297206710731\n",
      "[Loss]: 0.005463834842884008\n",
      "[Loss]: 0.005450483492548371\n",
      "[Loss]: 0.005437243128804622\n",
      "[Loss]: 0.005424113700818163\n",
      "[Loss]: 0.005411095110783132\n",
      "[Loss]: 0.005398187188993496\n",
      "[Loss]: 0.005385389668506134\n",
      "[Loss]: 0.0053727021610679975\n",
      "[Loss]: 0.005360124136064524\n",
      "[Loss]: 0.0053476549040769586\n",
      "[Loss]: 0.005335293606291194\n",
      "[Loss]: 0.00532303921049481\n",
      "[Loss]: 0.0053108905138188455\n",
      "[Loss]: 0.0052988461517920055\n",
      "[Loss]: 0.005286904612796868\n",
      "[Loss]: 0.005275064256690757\n",
      "[Loss]: 0.005263323336183981\n",
      "[Loss]: 0.005251680019613691\n",
      "[Loss]: 0.005240132413893262\n",
      "[Loss]: 0.0052286785866801485\n",
      "[Loss]: 0.005217316587086337\n",
      "[Loss]: 0.005206044464548503\n",
      "[Loss]: 0.0051948602856940074\n",
      "[Loss]: 0.0051837621492293855\n",
      "[Loss]: 0.005172748199005592\n",
      "[Loss]: 0.0051618166354608745\n",
      "[Loss]: 0.00515096572566984\n",
      "[Loss]: 0.00514019381222979\n",
      "[Loss]: 0.005129499321164793\n",
      "[Loss]: 0.0051188807690230355\n",
      "[Loss]: 0.005108336769283085\n",
      "[Loss]: 0.005097866038184178\n",
      "[Loss]: 0.00508746740005529\n",
      "[Loss]: 0.005077139792225491\n",
      "[Loss]: 0.0050668822695790275\n",
      "[Loss]: 0.0050566940088523004\n",
      "[Loss]: 0.005046574312736282\n",
      "[Loss]: 0.005036522613909995\n",
      "[Loss]: 0.00502653847912312\n",
      "[Loss]: 0.005016621613471752\n",
      "[Loss]: 0.005006771865036674\n",
      "[Loss]: 0.004996989230082438\n",
      "[Loss]: 0.004987273859048151\n",
      "[Loss]: 0.004977626063586813\n",
      "[Loss]: 0.004968046324947521\n",
      "[Loss]: 0.004958535304065885\n",
      "[Loss]: 0.0049490938537458855\n",
      "[Loss]: 0.00493972303342281\n",
      "[Loss]: 0.004930424127061697\n",
      "[Loss]: 0.004921198664854839\n",
      "[Loss]: 0.004912048449535055\n",
      "[Loss]: 0.004902975588276383\n",
      "[Loss]: 0.004893982531388026\n",
      "[Loss]: 0.004885072119291728\n",
      "[Loss]: 0.004876247639647295\n",
      "[Loss]: 0.004867512896950478\n",
      "[Loss]: 0.004858872297596744\n",
      "[Loss]: 0.004850330954181074\n",
      "[Loss]: 0.004841894813915011\n",
      "[Loss]: 0.0048335708174985495\n",
      "[Loss]: 0.004825367096685235\n",
      "[Loss]: 0.004817293221418978\n",
      "[Loss]: 0.004809360510935571\n",
      "[Loss]: 0.004801582428116314\n",
      "[Loss]: 0.0047939750830482235\n",
      "[Loss]: 0.004786557881167195\n",
      "[Loss]: 0.0047793543645487675\n",
      "[Loss]: 0.004772393313750827\n",
      "[Loss]: 0.0047657102046689175\n",
      "[Loss]: 0.004759349154085307\n",
      "[Loss]: 0.004753365544799685\n",
      "[Loss]: 0.004747829604583274\n",
      "[Loss]: 0.004742831333526852\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-2\n",
    "for e in range(100):\n",
    "    # compute loss\n",
    "    loss = 0\n",
    "    xs, ps, hs = {}, {}, {}\n",
    "    h = np.zeros((1, hidden_size))\n",
    "    hs[-1] = h\n",
    "    for t in range(len(inp)):\n",
    "        idx = inp[t]\n",
    "        x = np.zeros((1, vocab_size))\n",
    "        x[:, idx] = 1\n",
    "        probs, h = rnn(x, h)\n",
    "        xs[t] = x\n",
    "        ps[t] = probs # save probs (we'll use this in backprop)\n",
    "        hs[t] = h\n",
    "\n",
    "        # cross entropy\n",
    "        pred = probs[0, target[t]]\n",
    "        loss += -np.log(pred)\n",
    "\n",
    "    # backprop, calculate gradients\n",
    "    dL_dWxh = np.zeros_like(Wxh)\n",
    "    dL_dWhh = np.zeros_like(Whh)\n",
    "    dL_dbh = np.zeros_like(bh)\n",
    "    dL_Why = np.zeros_like(Why)\n",
    "    dL_dby = np.zeros_like(by)\n",
    "    dF_dh = np.zeros((1, hidden_size))\n",
    "\n",
    "    for t in reversed(range(len(inp))):\n",
    "        dL_dz2 = np.copy(ps[t])\n",
    "        target_idx = target[t]\n",
    "        dL_dz2[:, target_idx] -= 1\n",
    "\n",
    "        # print(dL_dz2.shape)\n",
    "\n",
    "        a = hs[t] * dL_dz2.T\n",
    "        # print(a.shape)\n",
    "\n",
    "        # 2nd layer\n",
    "        dL_Why += hs[t] * dL_dz2.T\n",
    "        dL_dby += dL_dz2\n",
    "\n",
    "\n",
    "        # 1st layer\n",
    "        dh_dz1 = 1 - hs[t] ** 2\n",
    "        dL_dh = dF_dh + (dL_dz2 @ Why) \n",
    "\n",
    "        # print((xs[t] * dh_dz1.T * dL_dh.T).shape)\n",
    "\n",
    "        dL_dWxh += xs[t] * dh_dz1.T * dL_dh.T\n",
    "        dL_dWhh +=  hs[t - 1] * dh_dz1.T * dL_dh.T\n",
    "        dL_dbh += dh_dz1 * dL_dh\n",
    "\n",
    "        # raise RuntimeError\n",
    "\n",
    "        # dF_dh = dL_dh * (dh_dz1 @ Whh)\n",
    "        dF_dh = dL_dh @ (dh_dz1 * Whh)\n",
    "\n",
    "    # clip gradients\n",
    "    for gradient in [dL_dWxh, dL_dWhh, dL_Why, dL_dbh, dL_dby]:\n",
    "        np.clip(gradient, -1, 1, out=gradient)\n",
    "        # np.clip(gradient, -5, 5, out=gradient)\n",
    "\n",
    "    # gradient descent\n",
    "    for weights, gradient in zip([Wxh, Whh, Why, bh, by], [dL_dWxh, dL_dWhh, dL_Why, dL_dbh, dL_dby]):\n",
    "        weights -= lr * gradient\n",
    "\n",
    "    # break\n",
    "    \n",
    "    print(f\"[Loss]: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ello worll'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(\"h\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "core-JXcGQI7U-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
