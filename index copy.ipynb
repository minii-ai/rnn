{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'S': 0,\n",
       "  'N': 1,\n",
       "  'a': 2,\n",
       "  't': 3,\n",
       "  'k': 4,\n",
       "  'M': 5,\n",
       "  'l': 6,\n",
       "  'r': 7,\n",
       "  ',': 8,\n",
       "  'h': 9,\n",
       "  'o': 10,\n",
       "  's': 11,\n",
       "  'u': 12,\n",
       "  'i': 13,\n",
       "  '\\n': 14,\n",
       "  'e': 15,\n",
       "  'W': 16,\n",
       "  ' ': 17,\n",
       "  'n': 18,\n",
       "  'y': 19},\n",
       " {0: 'S',\n",
       "  1: 'N',\n",
       "  2: 'a',\n",
       "  3: 't',\n",
       "  4: 'k',\n",
       "  5: 'M',\n",
       "  6: 'l',\n",
       "  7: 'r',\n",
       "  8: ',',\n",
       "  9: 'h',\n",
       "  10: 'o',\n",
       "  11: 's',\n",
       "  12: 'u',\n",
       "  13: 'i',\n",
       "  14: '\\n',\n",
       "  15: 'e',\n",
       "  16: 'W',\n",
       "  17: ' ',\n",
       "  18: 'n',\n",
       "  19: 'y'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = \"hello world\"\n",
    "data = open(\"./data/howtogetrich.txt\", \"r\").read()[:50]\n",
    "chars = list(set(data))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "chars_to_indices = {char: i for i, char in enumerate(chars)}\n",
    "indices_to_chars = {i: char for i, char in enumerate(chars)}\n",
    "\n",
    "chars_to_indices, indices_to_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN configs\n",
    "embedding_size = vocab_size\n",
    "hidden_size = 64\n",
    "output_size = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices\n",
    "Wxh = np.random.randn(hidden_size, embedding_size) * 0.01\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "bh = np.random.randn(1, hidden_size) * 0.01\n",
    "\n",
    "Why = np.random.randn(output_size, hidden_size)\n",
    "by = np.random.randn(1, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    return e_z / e_z.sum(axis=1)\n",
    "\n",
    "def rnn(x_t, h_prev):\n",
    "    assert x_t.shape == (1, embedding_size)\n",
    "    assert h_prev.shape == (1, hidden_size)\n",
    "\n",
    "    h_next = np.tanh(x_t @ Wxh.T + h_prev @ Whh.T + bh)\n",
    "    probs = softmax(h_next @ Why.T + by)\n",
    "    \n",
    "    return probs, h_next\n",
    "\n",
    "def sample(char, n):\n",
    "    # sample rnn n times starting with the first char\n",
    "    x = np.zeros((1, vocab_size))\n",
    "    idx = chars_to_indices[char]\n",
    "    x[:, idx] = 1 # one hot encoding\n",
    "    h =  np.zeros((1, hidden_size)) # hidden state\n",
    "    idxes = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        probs, h = rnn(x, h) # xt, hprev -> rnn -> probs, hnext\n",
    "        idx = np.random.choice(vocab_size, p=probs.ravel()) # .ravel returns a 1d array\n",
    "        \n",
    "        x = np.zeros((1, vocab_size))\n",
    "        x[:, idx] = 1\n",
    "        idxes.append(idx)\n",
    "\n",
    "    chars = \"\".join([indices_to_chars[i] for i in idxes])\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 20), (1, 64))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t = np.random.randn(1, embedding_size)\n",
    "h_prev = np.random.randn(1, hidden_size)\n",
    "\n",
    "probs, h_next = rnn(x_t, h_prev)\n",
    "probs.shape, h_next.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0,\n",
       "  15,\n",
       "  15,\n",
       "  4,\n",
       "  17,\n",
       "  16,\n",
       "  15,\n",
       "  2,\n",
       "  6,\n",
       "  3,\n",
       "  9,\n",
       "  8,\n",
       "  17,\n",
       "  1,\n",
       "  10,\n",
       "  3,\n",
       "  17,\n",
       "  5,\n",
       "  10,\n",
       "  18,\n",
       "  15,\n",
       "  19,\n",
       "  17,\n",
       "  10,\n",
       "  7,\n",
       "  17,\n",
       "  0,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  12,\n",
       "  11,\n",
       "  14,\n",
       "  16,\n",
       "  15,\n",
       "  2,\n",
       "  6,\n",
       "  3,\n",
       "  9,\n",
       "  17,\n",
       "  13,\n",
       "  11,\n",
       "  17,\n",
       "  2,\n",
       "  11,\n",
       "  11,\n",
       "  15,\n",
       "  3,\n",
       "  11],\n",
       " [15,\n",
       "  15,\n",
       "  4,\n",
       "  17,\n",
       "  16,\n",
       "  15,\n",
       "  2,\n",
       "  6,\n",
       "  3,\n",
       "  9,\n",
       "  8,\n",
       "  17,\n",
       "  1,\n",
       "  10,\n",
       "  3,\n",
       "  17,\n",
       "  5,\n",
       "  10,\n",
       "  18,\n",
       "  15,\n",
       "  19,\n",
       "  17,\n",
       "  10,\n",
       "  7,\n",
       "  17,\n",
       "  0,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  12,\n",
       "  11,\n",
       "  14,\n",
       "  16,\n",
       "  15,\n",
       "  2,\n",
       "  6,\n",
       "  3,\n",
       "  9,\n",
       "  17,\n",
       "  13,\n",
       "  11,\n",
       "  17,\n",
       "  2,\n",
       "  11,\n",
       "  11,\n",
       "  15,\n",
       "  3,\n",
       "  11,\n",
       "  17])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = [chars_to_indices[c] for c in data[:-1]]\n",
    "target = [chars_to_indices[c] for c in data[1:]]\n",
    "\n",
    "inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SS h'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(\"h\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loss]: 161.90099734913815\n",
      "[Loss]: 138.81123005923382\n",
      "[Loss]: 118.20969234860591\n",
      "[Loss]: 101.06409679720808\n",
      "[Loss]: 88.75276439609502\n",
      "[Loss]: 79.80701845619102\n",
      "[Loss]: 71.41177973703473\n",
      "[Loss]: 64.13376318393837\n",
      "[Loss]: 55.223823740104606\n",
      "[Loss]: 49.03456089604532\n",
      "[Loss]: 43.48175530622129\n",
      "[Loss]: 38.55246932739145\n",
      "[Loss]: 33.02879485535942\n",
      "[Loss]: 27.354741650024508\n",
      "[Loss]: 23.196315842466664\n",
      "[Loss]: 18.818272437090553\n",
      "[Loss]: 16.689283546573847\n",
      "[Loss]: 13.091161929333808\n",
      "[Loss]: 11.915546077180503\n",
      "[Loss]: 10.059517517927802\n",
      "[Loss]: 9.544689585368982\n",
      "[Loss]: 9.232571861940162\n",
      "[Loss]: 8.02530953735336\n",
      "[Loss]: 7.722991936946581\n",
      "[Loss]: 6.780050838243331\n",
      "[Loss]: 6.4746052950006145\n",
      "[Loss]: 5.867295849591302\n",
      "[Loss]: 5.673959068295171\n",
      "[Loss]: 5.20003705883193\n",
      "[Loss]: 5.127539730991191\n",
      "[Loss]: 4.723859057619936\n",
      "[Loss]: 4.713629903965679\n",
      "[Loss]: 4.36195600796531\n",
      "[Loss]: 4.372718381984839\n",
      "[Loss]: 4.066042805784638\n",
      "[Loss]: 4.08892969479131\n",
      "[Loss]: 3.816115060604503\n",
      "[Loss]: 3.849125078437209\n",
      "[Loss]: 3.6008415449626154\n",
      "[Loss]: 3.642885126403704\n",
      "[Loss]: 3.411994356379959\n",
      "[Loss]: 3.4619101178719505\n",
      "[Loss]: 3.2436571107000245\n",
      "[Loss]: 3.3010140728702217\n",
      "[Loss]: 3.0924830633972213\n",
      "[Loss]: 3.1558906503505737\n",
      "[Loss]: 2.9557299898168554\n",
      "[Loss]: 3.0224555873154335\n",
      "[Loss]: 2.8304084704208954\n",
      "[Loss]: 2.8968613622831167\n",
      "[Loss]: 2.713936565434128\n",
      "[Loss]: 2.77468001424719\n",
      "[Loss]: 2.602298659241324\n",
      "[Loss]: 2.653144717804817\n",
      "[Loss]: 2.4910276094667423\n",
      "[Loss]: 2.529301825720026\n",
      "[Loss]: 2.3743113178131012\n",
      "[Loss]: 2.39708687927478\n",
      "[Loss]: 2.243682416851578\n",
      "[Loss]: 2.2392660142688277\n",
      "[Loss]: 2.071035296838962\n",
      "[Loss]: 2.0058879641458836\n",
      "[Loss]: 1.7899852252377106\n",
      "[Loss]: 1.577834442335729\n",
      "[Loss]: 1.2134017169557418\n",
      "[Loss]: 0.7766920032262996\n",
      "[Loss]: 0.555709998097993\n",
      "[Loss]: 0.5156254438993196\n",
      "[Loss]: 0.4878019909086973\n",
      "[Loss]: 0.46443151215637446\n",
      "[Loss]: 0.44402098389948863\n",
      "[Loss]: 0.42588928673767024\n",
      "[Loss]: 0.4096040192196074\n",
      "[Loss]: 0.39485260308965986\n",
      "[Loss]: 0.3813953103609147\n",
      "[Loss]: 0.36904251480124717\n",
      "[Loss]: 0.35764142914868075\n",
      "[Loss]: 0.34706739165559347\n",
      "[Loss]: 0.3372176984478755\n",
      "[Loss]: 0.32800703335027803\n",
      "[Loss]: 0.3193639849052645\n",
      "[Loss]: 0.3112283451960479\n",
      "[Loss]: 0.30354899143417713\n",
      "[Loss]: 0.2962822119441032\n",
      "[Loss]: 0.28939037586046756\n",
      "[Loss]: 0.28284087101651395\n",
      "[Loss]: 0.27660525228382554\n",
      "[Loss]: 0.270658555709024\n",
      "[Loss]: 0.26497874368089397\n",
      "[Loss]: 0.25954625395392206\n",
      "[Loss]: 0.2543436312366779\n",
      "[Loss]: 0.24935522463244134\n",
      "[Loss]: 0.24456693779137148\n",
      "[Loss]: 0.23996602142285153\n",
      "[Loss]: 0.23554089999596875\n",
      "[Loss]: 0.23128102615997703\n",
      "[Loss]: 0.22717675774970258\n",
      "[Loss]: 0.22321925328553743\n",
      "[Loss]: 0.21940038269729342\n",
      "[Loss]: 0.21571265064611872\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-2\n",
    "for e in range(100):\n",
    "    # compute loss\n",
    "    loss = 0\n",
    "    xs, ps, hs = {}, {}, {}\n",
    "    h = np.zeros((1, hidden_size))\n",
    "    hs[-1] = h\n",
    "    for t in range(len(inp)):\n",
    "        idx = inp[t]\n",
    "        x = np.zeros((1, vocab_size))\n",
    "        x[:, idx] = 1\n",
    "        probs, h = rnn(x, h)\n",
    "        xs[t] = x\n",
    "        ps[t] = probs # save probs (we'll use this in backprop)\n",
    "        hs[t] = h\n",
    "\n",
    "        # cross entropy\n",
    "        pred = probs[0, target[t]]\n",
    "        loss += -np.log(pred)\n",
    "\n",
    "    # backprop, calculate gradients\n",
    "    dL_dWxh = np.zeros_like(Wxh)\n",
    "    dL_dWhh = np.zeros_like(Whh)\n",
    "    dL_dbh = np.zeros_like(bh)\n",
    "    dL_Why = np.zeros_like(Why)\n",
    "    dL_dby = np.zeros_like(by)\n",
    "    dF_dh = np.zeros((1, hidden_size))\n",
    "\n",
    "    for t in reversed(range(len(inp))):\n",
    "        dL_dz2 = np.copy(ps[t])\n",
    "        target_idx = target[t]\n",
    "        dL_dz2[:, target_idx] -= 1\n",
    "\n",
    "        # print(dL_dz2.shape)\n",
    "\n",
    "        a = hs[t] * dL_dz2.T\n",
    "        # print(a.shape)\n",
    "\n",
    "        # 2nd layer\n",
    "        dL_Why += hs[t] * dL_dz2.T\n",
    "        dL_dby += dL_dz2\n",
    "\n",
    "\n",
    "        # 1st layer\n",
    "        dh_dz1 = 1 - hs[t] ** 2\n",
    "        dL_dh = dF_dh + (dL_dz2 @ Why) \n",
    "\n",
    "        # print((xs[t] * dh_dz1.T * dL_dh.T).shape)\n",
    "\n",
    "        dL_dWxh += xs[t] * dh_dz1.T * dL_dh.T\n",
    "        dL_dWhh +=  hs[t - 1] * dh_dz1.T * dL_dh.T\n",
    "        dL_dbh += dh_dz1 * dL_dh\n",
    "\n",
    "        # raise RuntimeError\n",
    "\n",
    "        # dF_dh = dL_dh * (dh_dz1 @ Whh)\n",
    "        dF_dh = dL_dh @ (dh_dz1 * Whh)\n",
    "\n",
    "    # clip gradients\n",
    "    for gradient in [dL_dWxh, dL_dWhh, dL_Why, dL_dbh, dL_dby]:\n",
    "        np.clip(gradient, -1, 1, out=gradient)\n",
    "        # np.clip(gradient, -5, 5, out=gradient)\n",
    "\n",
    "    # gradient descent\n",
    "    for weights, gradient in zip([Wxh, Whh, Why, bh, by], [dL_dWxh, dL_dWhh, dL_Why, dL_dbh, dL_dby]):\n",
    "        weights -= lr * gradient\n",
    "\n",
    "    # break\n",
    "    \n",
    "    print(f\"[Loss]: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eek Wealth, Not Money or Status\\nWealth is assets W'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(\"S\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "core-JXcGQI7U-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
